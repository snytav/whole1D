# -*- coding: utf-8 -*-
"""Whole Domain Poisson1D.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-5pER7MxZuXnTZDF0ZsmDHmjFhTLNCIT
"""

#import torch
nx = 3

def f(x, psy, dpsy):
    '''
        d2(psy)/dx2 = f(x, dpsy/dx, psy)
        This is f() function on the right
    '''
    return -1./5. * torch.exp(-x/5.) * torch.cos(x) - 1./5. * dpsy - psy


def psy_analytic(x):
    '''
        Analytical solution of current problem
    '''
    return torch.exp(-x/5.) * torch.sin(x)

from torch.autograd.functional import jacobian,hessian

f = lambda x: torch.sin(x)

import torch
x_space =torch.linspace(0, 2, nx,requires_grad=True)
y_space = psy_analytic(x_space)
x =x_space

def psy_trial(xi, net_out):
    return xi + xi**2 * net_out

import torch.nn as nn
from multiply import Multiply
import torch
class PoisNet(nn.Module):
    def __init__(self,nx):
        super(PoisNet,self).__init__()
        self.nx = nx
        fc1 = Multiply(1,self.nx)
        self.fc1 = fc1
        fc2 = nn.Linear(self.nx,1)
        self.fc2 = fc2
    def forward(self,x): # at first place replace forward with exact solution
        return torch.sin(x)
                         # to check loss function
        x = self.fc1(x)
        x = torch.sigmoid(x)
        x = self.fc2(x)
        return x
    def prime(self,x):
        j=jacobian(self.forward,inputs=x)
        return j.diag()

# pn= PoisNet(nx)
#
# x
#
# net_out = pn.forward(x)
# net_out
#
# j = jacobian(pn.forward,inputs=x)
# df_dx = j.diag()
# df_dx
#
# p = pn.prime(x)
#
# jj = jacobian(pn.prime,inputs=x)
# jj
#
# import torch
# torch.max(torch.abs(torch.cos(x)-df_dx))

#h = jacobian(pn.ÑŠ,inputs=x)

# def loss_function(W, x):
#     loss_sum = 0.
#
#
#
#
#     net_out_d = grad(neural_network_x)(xi)
#     net_out_dd = grad(grad(neural_network_x))(xi)
#
#     psy_t = psy_trial(xi, net_out)
#
#     # gradient_of_trial = psy_grad(xi, net_out)
#     # second_gradient_of_trial = psy_grad2(xi, net_out)
#
#     # func = f(xi, psy_t, gradient_of_trial) # right part function
#     #
#     # err_sqr = (second_gradient_of_trial - func)**2
#     loss_sum += err_sqr

#    return loss_sum